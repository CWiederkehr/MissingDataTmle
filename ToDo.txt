Github-repo:

Struktur:

model-based:
+01_general_functions.R   # Utility functions used across the project
+02_data_generation.R     # Scripts to simulate or load data
+03_inducing_missingness.R # Procedures to create missing data
+04_analysis.R            # Analysis routines, model fitting, etc.
+05_plots.R           # Visualization scripts
+intermediate_results
+Prior_results # provide a calulation of how Long one single apply_TMLE would take (don`t run tmle, just run plots with results and Maybe data gen&missing)
###########-----> would it also go with n=1000 or n=500 ? or will it scre my pos.violation?

design-based:
+ General_function  # Need to clarify that i use the functions of "Li" from his paper -> some add ons etc.
+ Data 
+ missingnes
+ Analysis 
+ plots
+intermediate_results
+Prior_results

+data



Problems to handle:
- i will need to restore the datasets into lists  (also missingness overview table, ps vio Graphs, just the easy results auto-saved!)
- These lists have to be accessable via a for loop and then the 'apply_MI' function has to run through
- for each m-Dag the list-Name should be "mDagT_DGP1_sce1_res"
- if i apply my 'all_measure' function then i need to run over the for loop and loop over the lists, but i Need to Change
  how the names are handled -> this is something i Need to check by Hand
- If the big table is generated then i can apply easyly the plot functions

-> Intermediate Results:
i need to provide my intermediate Results, because its impossible for them to run simulation (needs days)



Checkpoint:
-make check if possible on LRZ Cloud for one single run on m=10 and 1000 datasets, and check if results are still same
---> if so, then save the r-Version and all libraries!  (Put the notes in the library file)

Einsparpotential:
- analysis function -> Tmle auslagern (aber ist jetzt nicht so viel, was man sich spart)
 


-------------------------------
Paper-Work:

+ Paper MI-part: Change everything to PMM  (but the categorical variable? -> should be stated)
   -> maybe also important to note, that we implemented also the outcome for our imputation models
   -> small table for all the interactions necessary?
+ also is it checkable that we used m=10 instead of 100? by simulation with seed=5 yes; otherwise no

+ Hal-TheorieTeil direkt zur modelbased simulation packen
  -> wie positivität unterbringen? Prinzipiell ist es redundant im Haupteil, dass so zu lassen. Es reicht auch einfach nur das im Anhang ausfürhlicher zu erwähnen
+ paar unterchiede zu Dashti?


Offene Punkte:

-SL-Mice für congeniality in Kombination mit SL in TMLE
-Results und Conclusion kürzer?
- Noch bisschen mehr zu RMSE?





Main critical idea: (might be to offensive, i am lacking the confidence to actually state this)
- In the literature it is often stated that MI should be prefered over CCA not just because of ist Efficiency but also because 
  CC provides larger bias
- There are a few papers which also underline the Benefits of CCA or some appropriate use cases for CC
- Nevertheless if we consider more advanced multivariable missingness mechanism it becomes more eveident that the assumptions to be valid for applying MI
  are actually rarly met.
- Even so in the m-DAGs where MI should be valid it did worse than certain non-MI Methods or CCA 
-> main reason for this is that it is not really sure whether congeniality is actually provided for SUperLearner Methods
-> Efficiency is true according all sceanrios and also better Coverage, but regarding bias, MI is not really the best choice, especially under positivity Violation


--> Here it would be possible to Claim that Mice with SL would be a great future project




Future Research:  -might incoporate MI with SuperLearner
                  - has been shown that MI with SuperLearner provides less biased estimates and better Coverage than MI CART and PMM
                  - should be congenial with TMLE and SuperLearner, would be interesting to see if the best predictive model is better than the congenial Library
                  - However it is hard to implement because of high computational demand for variance estimation
